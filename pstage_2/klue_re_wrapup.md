> ì €í¬ì˜ [íŒ€ë¸”ë¡œê·¸](https://cheonggyemountain-sherpa.github.io/KLUE_RE/) ë¡œ ë³´ì‹œë©´ ë³´ë‹¤ ê°€ë…ì„± ìˆê³  ì´ìœ í™˜ê²½ì—ì„œ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

# ì²­ê³„ì‚°ì…°ë¥´íŒŒì˜ KLUE-RE ë“±ë°˜ì¼ì§€

ì•½ 12ì¼ë™ì•ˆ ì°¸ì—¬í–ˆë˜ P Stageì˜ `KLUE - Relation Extraction` ëŒ€íšŒì˜ Wrap up Reportì´ì íšŒê³ ë¡ì…ë‹ˆë‹¤. ì² ì €í•œ `ê¸°ë¡`ê³¼ `ê³µìœ `ì— ê³µê°í•œ 7ëª…ì˜ íŒ€ì›ì´ ì²˜ìŒ í•©ì„ ë§ì¶”ì—ˆê¸° ë•Œë¬¸ì— ì–´ì„¤í”ˆ ì ë„ ìˆì—ˆì§€ë§Œ, ê·¸ê°„ì˜ ìƒìƒí•œ ê¸°ë¡ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ë–»ê²Œ ì„œë¡œê°€ ì„œë¡œì˜ `ì…°ë¥´íŒŒ`ë¡œì¨ ë“±ë°˜ì„ ì™„ë£Œí•  ìˆ˜ ìˆì—ˆëŠ”ì§€ ì €í¬ì˜ ê²½í—˜ì„ ë‚˜ëˆ„ê³ ì í•©ë‹ˆë‹¤.

ë¬¼ë¡  ë¯¸í¡í•œ ì ë„ ìˆì—ˆê³ , ì—¬ëŸ¬ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªì—ˆì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œëŠ” íŒ€ì› ëª¨ë‘ê°€ ë§Œì¡±í•œ í˜‘ì—…ì´ì—ˆë‹¤ëŠ” ì ì—ì„œ ì§€ë‚œ ëŒ€íšŒë³´ë‹¤ ë§ì€ ì„±ì¥ì„ í•  ìˆ˜ ìˆì—ˆë‹¤ê³  ëŠë‚„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì €í¬ì˜ ê²½í—˜ì´ ì–´ë– í•œ í˜•íƒœë¡œë“  ë„ì›€ì´ ë˜ê³ , ì¢‹ì€ ë ˆí¼ëŸ°ìŠ¤ê°€ ë˜ê¸°ë¥¼ ë°”ë¼ë©° ì‹œì‘í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br>

**ëª©ì°¨**

- [íŒ€ì†Œê°œ](#%E2%9A%94%EF%B8%8F-%ED%8C%80-%EC%86%8C%EA%B0%9C)
    - [ì²­ê³„ì‚°ì…°ë¥´íŒŒ](#%F0%9F%A4%9C-%EB%B6%80%EB%A1%9D-%ED%8C%80%EC%9B%90%EB%93%A4-%ED%95%9C%EB%A7%88%EB%94%94)
    - [ì²­ê³„ì‚°ì…°ë¥´íŒŒë“¤](#%F0%9F%91%A8%E2%80%8D%F0%9F%91%A8%E2%80%8D%F0%9F%91%A6%E2%80%8D%F0%9F%91%A6-%EC%B2%AD%EA%B3%84%EC%82%B0%EC%85%B0%EB%A5%B4%ED%8C%8C%EB%93%A4)
- [ëŒ€íšŒê°œìš”](#%F0%9F%94%8E-%EB%8C%80%ED%9A%8C-%EA%B0%9C%EC%9A%94)
- [í˜‘ì—…](#%F0%9F%A4%9D-%ED%98%91%EC%97%85)
    - [ì‚¬ì „ë…¼ì˜](#%F0%9F%A5%BE-%EC%82%AC%EC%A0%84%EB%85%BC%EC%9D%98)
    - [í˜‘ì—…íˆ´](#%E2%9A%99%EF%B8%8F-%ED%98%91%EC%97%85%ED%88%B4)
    - [ì½”ë“œê´€ë¦¬](#%F0%9F%92%BB-%EC%BD%94%EB%93%9C-%EA%B4%80%EB%A6%AC)
    - [ì²´ê³„ì ì¸ ì‹¤í—˜](#%F0%9F%A7%91%E2%80%8D%F0%9F%94%AC-%EC%B2%B4%EA%B3%84%EC%A0%81%EC%9D%B8-%EC%8B%A4%ED%97%98)
- [Data Experiments](#%F0%9F%9B%8B-Data-Experiments)
    - [Data EDA](#%F0%9F%91%81-Data-EDA)
    - [Data Augmentation](#%F0%9F%92%AA-Data-Augmentation)
    - [Data Preprocessing](#%F0%9F%94%A7-Data-Preprocessing-amp-Tokenizer)
- [Modeling](#%F0%9F%A7%98-Modeling)
- [Ensemble](#%F0%9F%8D%9C-Ensemble)
    - [ì•™ìƒë¸” ê¹ëŠ” ë…¸ì¸ê³¼ ê¸°ë„ë©”íƒ€](#%F0%9F%91%A8%E2%80%8D%F0%9F%8E%A8-%EC%95%99%EC%83%81%EB%B8%94-%EA%B9%8E%EB%8A%94-%EB%85%B8%EC%9D%B8%EA%B3%BC-%EA%B8%B0%EB%8F%84%EB%A9%94%ED%83%80)
- [Good Practice](#%F0%9F%92%AF-Good-Practice)
- [Thanks to](#%F0%9F%92%8C-Thanks-to)
- [ë§ˆì§€ë§‰ìœ¼ë¡œ](#%EB%A7%88%EC%A7%80%EB%A7%89%EC%9C%BC%EB%A1%9C)
- [ë¶€ë¡: íŒ€ì›ë“¤ í•œë§ˆë””](#%F0%9F%A4%9C-%EB%B6%80%EB%A1%9D-%ED%8C%80%EC%9B%90%EB%93%A4-%ED%95%9C%EB%A7%88%EB%94%94)


## âš”ï¸ íŒ€ ì†Œê°œ

### â›° ì²­ê³„ì‚°ì…°ë¥´íŒŒ
<p align="center">
    <img src="/images/profile.png" style="display: inline" height="120px">
    <img src="/images/2.png" style="display: inline" height="120px">
</p>

ì €í¬ëŠ” ìº í”„ ê¸°ê°„ë™ì•ˆ ëª¨ë“  ê²ƒì„ ìƒìƒí•˜ê²Œ ê¸°ì–µí•˜ê³  ë‚˜ëˆ„ëŠ” `ê¸°ë¡`ê³¼ `ê³µìœ `ë¼ëŠ” ê°€ì¹˜ì— ê³µê°í•œ 7ëª…ì´ ëª¨ì—¬ íŒ€ì„ êµ¬ì„±í–ˆê³ , ì„œë¡œê°€ ì„œë¡œì˜ ê°€ì´ë“œë¡œì„œ ì¢‹ì€ ì˜í–¥ì„ ì£¼ê³ ë°›ì„ ìˆ˜ ìˆëŠ” ì…°ë¥´íŒŒê°€ ë˜ê¸°ë¥¼ ì›í–ˆìŠµë‹ˆë‹¤.

ë˜í•œ ì£¼ë‹ˆì–´ ì—”ì§€ë‹ˆì–´ë“¤ì˜ ë¡œë§ì€ íŒêµì—­ ê·¼ì²˜ íšŒì‚¬ë“¤ì—ì„œ ì¼ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì €í¬ëŠ” íŒêµì—­ì˜ ë’·ì‚°ì¸ ì²­ê³„ì‚°ì„ ë¶€ìŠ¤íŠ¸ìº í”„ ê³¼ì •ì— ë¹—ëŒ€ì–´ ì™„ë²½í•˜ê²Œ ë“±ë°˜í•´ë³´ê² ë‹¤ëŠ” ì˜ë¯¸ë¡œ ì²­ê³„ì‚°ê³¼ ì…°ë¥´íŒŒë¥¼ ë”í•´ `ì²­ê³„ì‚°ì…°ë¥´íŒŒ`ë¼ëŠ” ì´ë¦„ì„ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.


### ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ì²­ê³„ì‚°ì…°ë¥´íŒŒë“¤

- <a href="https://github.com/l-yohai">
<img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/49181231?s=96&amp;v=4" width="20px"/>
</a> &nbsp; ì´ìš”í•œ

- <a href="https://github.com/ddobokki">
    <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/44228269?s=96&v=4" width="20px">
</a> &nbsp; ë¬¸í•˜ê²¸

- <a href="https://github.com/20180707jun">
    <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/50571795?s=96&v=4" width="20px">
</a> &nbsp; ì „ì¤€ì˜

- <a href="https://github.com/godjw">
    <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/47168115?s=96&v=4" width="20px">
</a> &nbsp; ì •ì§„ì›

- <a href="https://github.com/lexiconium">
    <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/84180121?s=96&v=4" width="20px">
</a> &nbsp; ê¹€ë¯¼ìˆ˜

- <a href="https://github.com/hyeong01">
    <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/38185429?s=96&v=4" width="20px">
</a> &nbsp; ì •í¬ì˜

- <a href="https://github.com/jskwak98">
    <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/47588410?s=96&v=4" width="20px">
</a> &nbsp; ê³½ì§„ì„±


## ğŸ” ëŒ€íšŒ ê°œìš”

ì´ë²ˆì— ì°¸ì—¬í•œ ëŒ€íšŒì˜ ê³¼ì œëŠ” ë¬¸ì¥ ë‚´ ê°œì²´ê°„ ê´€ê³„ ì¶”ì¶œ ê³¼ì œë¡œ, ë¬¸ì¥ì˜ ë‹¨ì–´(Entity)ì— ëŒ€í•œ ì†ì„±ê³¼ ê´€ê³„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ê³¼ì œì…ë‹ˆë‹¤. ê´€ê³„ ì¶”ì¶œì€ ë¹„êµ¬ì¡°ì ì¸ ìì—°ì–´ ë¬¸ì¥ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°ì— ëª©ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.

ì €í¬ê°€ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ [ã€ŒKLUE: Korean Language Understanding Evaluationã€(Park et al., 2021)](https://arxiv.org/pdf/2105.09680.pdf)ì„ í†µí•´ ê³µê°œëœ KLUE-RE ë°ì´í„°ì…‹ìœ¼ë¡œ, KLUE ë°ì´í„°ì…‹ì„ ì‚¬ì „í•™ìŠµí•œ RoBERTaë¥¼ fine-tuningí•˜ì—¬ ë² ì´ìŠ¤ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, private leaderboardì—ì„œ f1 score ê¸°ì¤€ 19íŒ€ ì¤‘ 5ë“±, auprc ê¸°ì¤€ 19íŒ€ ì¤‘ 2ë“±ì„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.

ğŸ¥ˆ Final Score
![](/images/final.png)
- micro F1 score: 73.732 (19íŒ€ ì¤‘ **5ë“±**)
- AUPRC score: 82.964 (19íŒ€ ì¤‘ **2ë“±**)

![ë¬˜ë¹„ì•„ë‹˜](/images/kakaotalk.png)


## ğŸ¤ í˜‘ì—…

### ğŸ¥¾ ì‚¬ì „ë…¼ì˜

RE ëŒ€íšŒê°€ ì‹œì‘í•˜ê¸° 1ì£¼ì¼ ì „ë¶€í„°, ì €í¬ëŠ” ë¯¸ë¦¬ ëŒ€íšŒë¥¼ ìœ„í•œ ì „ëµë“¤ì„ êµ¬ìƒí–ˆìŠµë‹ˆë‹¤. ì„œë¡œì˜ ìŠ¤íƒ€ì¼ì„ ëª¨ë¥´ê¸°ì— ë‹¹ì¥ì€ ê²°ì •ì´ ì•ˆë‚˜ë”ë¼ë„, ì„¸ë¶€ì ì¸ ì‚¬í•­ë“¤ì— ëŒ€í•œ ë…¼ì˜ë¥¼ ë‹¨ í•œë²ˆì´ë¼ë„ ê±°ì³¤ë˜ ê²ƒì€ ì¶”í›„ì— ì˜ì‚¬ê²°ì •ì†ë„ì™€ íŒ€ì›ë“¤ì˜ ë§Œì¡±ì— ìˆì–´ì„œ í° ì˜í–¥ì„ ë¼ì³¤ìŠµë‹ˆë‹¤.

<details>
    <summary>
    ëŒ€íšŒ ì „ ë…¼ì˜ì‚¬í•­ë“¤
    </summary>
<br>

- í”„ë¡œì íŠ¸ê´€ë¦¬ íˆ´/ì±„ë„ ì •í•˜ê¸°
    - ex) Github projectì˜ kanban board, notion, slack, zoom, google meet, git page, github action ë“±ë“±
    - ì¹´ì¹´ì˜¤í†¡ (ìŠ¬ë™ë³´ë‹¤ ë§ì´ ì ‘í•¨.) ìŠ¬ë™ì²˜ëŸ¼ ìŠ¤ë ˆë“œ í˜•ì‹ì˜ ëŒ€í™”ë¥¼ í•  ìˆ˜ ì—†ìŒ.
        - ì¤‘ìš”í•œ ì´ìŠˆê°€ ìƒê¸°ë©´ ìŠ¬ë™ì—ë„ ê°™ì´ ì´ì•¼ê¸°í•˜ì.
- ì½”ë“œëª…ì„¸ì„œ or ì»¨ë²¤ì…˜
    - naming
        - í´ë˜ìŠ¤, Static Vars = CamelCase
        - ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª… = snake_case
    - formatting (& auto formatter)
        - autopep8
        - black, yap...
    - annotation
        - ''' docstring '''
        - VSCODE CODE_ANCHORE
            - TODO, NOTE,
        - docstringì„ ì•Œì•„ì„œ í•´ì£¼ëŠ” ê²Œ ìˆëŠ”ì§€ ì„œì¹˜í•´ë³´ê¸°!
        - í•„ìš”í•œ ë‚´ìš©ë§Œ ì‘ì„±í•  ìˆ˜ ìˆë„ë¡ ë£°ì„ ì¶”ê°€ë¡œ ì •í•  ê²ƒ.
        ì£¼ì„ì€ ì˜ë¬¸ìœ¼ë¡œ!!
    - ~~indentation (tab 1 or space 4)~~
    - ê·¸ ì™¸ vscode extensions
        - git graph, git lens
        - Live share
- ê°€ìƒí™˜ê²½ì´ë‚˜ í™˜ê²½ê´€ë¦¬ ì „ëµ (zsh, dotenv, â€¦ conda, pip â€¦)
    - **conda ì‚¬ìš©!**

- **ë¯¸ì •**

    - ë¸Œëœì¹˜ë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ì–´ë†“ì„ê²ƒì¸ê°€ (ì‹¤í—˜ì „ëµê¹Œì§€ ê³ ë ¤)
        ë‹¤ë¥¸ì‚¬ëŒë“¤ì´ checkoutë§Œìœ¼ë¡œ ë™ì¼í•œ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•¨
        MAIN
        - DEVELOP
                ìœ ì €í¸ì˜ì„±, ì½”ë“œê¸°ëŠ¥ê°œì„ , ë²„ê·¸í”½ìŠ¤
                ì ‘ë‘ì–´/feature
        - BASELINE
            ì ‘ë‘ì–´/feature x ì‹¤í—˜ o
        ë‹¤ë¥¸ íƒœìŠ¤í¬ì— ëŒ€í•œ ê²ƒë“¤ì´ë‚˜ (ì‹¤í—˜ìœ„ì£¼)
            baseline/qa/1
            baseline/ner
            baseline/sentence classification
    - ë„ˆë¬´ ë¸Œëœì¹˜ê°€ ë§ì•„ì§ˆ ìˆ˜ ìˆë‹¤.
        - configë¥¼ ë³€ê²½í•˜ë©´ ë¸Œëœì¹˜ë¥¼ ë¶„ê¸°í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

- (ì„ íƒì‚¬í•­) ì—¬ìœ ê°€ ë˜ë©´ ìì„¸í•˜ê²Œ ì¨ë†“ê¸°

- ì»¤ë°‹ì „ëµ
    - commit message ê·œì¹™ì´ë‚˜ í…œí”Œë¦¿ ì •í•˜ê¸°

- (ì‘ì—…ë‹¨ìœ„ì™€ ë¦¬ë·°ì— ëŒ€í•´ì„œ ë” ìƒê°í•´ë³´ê¸°)

- PR í…œí”Œë¦¿ ë° PR/Review ì „ëµ êµ¬ì²´í™”
    - PR ì˜¬ë¦¬ëŠ” íƒ€ì´ë°(ì‹œì )
        - ì„±ëŠ¥í–¥ìƒì— ì˜í•œ PRì€ ëª¨ë‘ê°€ ë¦¬ë·°
        - ìì˜í•œ ë³€í™”ë“¤ì€ ì±…ì„ìë§Œ ë¦¬ë·°
    - ë¦¬ë·°ë¥¼ ì–´ë–»ê²Œ í•  ê±´ì§€? ë¦¬ë·°ì–´ëŠ” ëª‡ ëª…
    - ì†ë„? Mergeì†ë„ì— ëŒ€í•´ì„œ ë°ë“œë¼ì¸ì´ ìˆì—ˆìŒ ì¢‹ê² ë‹¤.
    

- ê·¸ë¼ìš´ë“œë£° ë° ë¹ ë¥¸ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ í˜‘ì—…ê°€ì¹˜ ë¦¬ìŠ¤íŠ¸ì—… ë° ìš°ì„ ìˆœìœ„ ì •í•˜ê¸°
    - ex) ì•ˆì •ì„±(ì˜ˆì™¸ì²˜ë¦¬), êµ¬ì„±ì›ì˜ë§Œì¡±, ê°€ë…ì„±, ì¼ê´€ì„±, ê°ì²´ì§€í–¥ì„±, ë‹¨ìˆœì„±, ì™¸ë¶€ìœ ì €ì˜ê²½í—˜, ì‹ ì†ì„±(ì‘ì—…ì†ë„), í†µì œê°€ëŠ¥ì„±, í•™ìŠµê°€ëŠ¥ì„±, ì·¨ì—…ì ìš©ê°€ëŠ¥ì„±, ì‹¤í—˜ê°€ëŠ¥ì„± ë“±ë“±

</details>

ë˜í•œ ì´ëŸ¬í•œ ì‚¬ì „ë…¼ì˜ë¥¼ í†µí•´ ëŒ€íšŒê°€ ì§„í–‰ë ìˆ˜ë¡ íŒ€ì›ë“¤ì´ ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ í™•ì‹¤íˆ ì•Œ ìˆ˜ ìˆì—ˆìœ¼ë©°, ëª¨ë‘ê°€ ê°™ì€ ê·¸ë¦¼ì„ ê·¸ë¦¬ê³  ê°™ì€ ë°©í–¥ì„ ê°€ì§€ê³  ëŒ€íšŒì— ì„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ë…¸ë ¥ì„ í† ëŒ€ë¡œ ì €í¬ëŠ” ë¹ˆí‹ˆì—†ì´ ê¸°ë¡ì„ ì´ì–´ê°€ê³ , ì‹¤í—˜ì„ ê³µìœ í•˜ë©° í•­ìƒ ìƒˆë¡œìš´ ì‹¤í—˜ì•„ì´ë””ì–´ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ í•˜ë£¨ì— 10ë²ˆì”© ì œì¶œê°€ëŠ¥í•œ ëŒ€íšŒì—ì„œ 12ì¼ ë™ì•ˆ ë‹¤ë¥¸ íŒ€ë“¤ ëŒ€ë¹„ ì••ë„ì ì¸ íšŸìˆ˜ì¸ ***99ë²ˆì˜ ì‹¤í—˜ê²°ê³¼ë¥¼ ì œì¶œ*** í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

### âš™ï¸ í˜‘ì—…íˆ´

ì§€ë‚œ ëŒ€íšŒ ë•Œ ë‹¤ì–‘í•œ í˜‘ì—… ì±„ë„ì„ êµ¬ì„±í•˜ì˜€ì„ ë•Œ ì˜¤íˆë ¤ í˜¼ë€ì´ ê°€ì¤‘ë˜ê³  ëª¨ë“  ì±„ë„ì„ ì‚¬ìš©í•˜ê¸° í˜ë“¤ë‹¤ëŠ” ì ì„ ê³ ë ¤í•˜ì—¬, ëŒ€ë¶€ë¶„ì˜ í˜‘ì—…ì„ `Notion`ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ë¶ˆê°€í”¼í•œ ìƒí™©ì— ëŒ€ë¹„í•œ ì—°ë½ìˆ˜ë‹¨ìœ¼ë¡œ `kakaotalk`ê³¼ `slack`ì„ ì‚¬ìš©í•˜ì˜€ê³ , `git`ì€ ì‚¬ìš©ê°€ëŠ¥í•œ ê¸°ëŠ¥ì„ ìµœì†Œí•œìœ¼ë¡œ í•˜ê³  ì½”ë“œê´€ë¦¬ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

`Notion`ì—ì„œ ì¼ì •ê´€ë¦¬, ë¬¸ì„œê´€ë¦¬, ì‹¤í—˜ê´€ë¦¬ ë“± ë§ì€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì—„ë°€í•˜ê²Œ Templateì„ ì°¾ì•„ë³´ë©° Dashboardë¥¼ êµ¬ì„±í•˜ì˜€ê³  ê·¸ë ‡ê²Œ ì•„ë˜ì™€ ê°™ì€ í˜ì´ì§€ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

<p align="center">
    <img src="/images/notion_main.png" style="display: inline" height="220px" width="32%">
    <img src="/images/notion_kanban.png" style="display: inline" height="220px" width="32%">
    <img src="/images/notion_schedule.png" style="display: inline" height="220px" width="32%">
</p>

**Main Dashboard**
- sub pageë“¤ì˜ linkì™€ zoom, wandb, github, drive ë§í¬ë“¤
- TODOì™€ ì‹¤í—˜ê´€ë¦¬ë¥¼ ìœ„í•œ kanban ë³´ë“œ
    - ì‹¤í—˜ì„ ìœ„í•œ Processë³„ Tag ë¶€ì°©

        <img src="/images/no_tag.png" height="30%" width="30%">

    - Assignee í• ë‹¹
- ì¼ì •ê´€ë¦¬ë¥¼ ìœ„í•œ schedule
    - ì•Œë¦¼ê¸°ëŠ¥ í™œì„±í™”
    - ìš©ë„ë³„ Tag ë¶€ì°©
- Referenceì™€ Docs ë§í¬ë“¤
    - íšŒì˜ë¡
    - ë©˜í† ë§
    - ì—°êµ¬ì¼ì§€ ë“±

### ğŸ’» ì½”ë“œ ê´€ë¦¬

![](/images/git1.png)

ì´ˆë°˜ì—ëŠ” ì „ì²´ ì½”ë“œë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ PR-Merge ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•˜ë‹¤ê°€ Reviewê°€ ëŠ¦ì–´ì§€ê±°ë‚˜, ì‘ì—…ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ë©´ ë‹¤ë¥¸ íŒ€ì›ì´ ê°™ì€ ì‘ì—…ì„ í•˜ëŠ” ë“± ì˜ˆìƒì¹˜ ëª»í•œ ë³‘ëª©ì´ ë°œìƒí•˜ê³  ì˜¤íˆë ¤ ê°œì¸ ì‹¤í—˜ì— ë°©í•´ìš”ì†Œë¡œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” íŒë‹¨ì„ í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

<style>
  .linear_highlight {
      background: linear-gradient(to top, #778899 10%, transparent 10%);
  }
</style>

<span class="linear_highlight">
ë”°ë¼ì„œ baselineìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì½”ë“œì—ì„œ ê°ìì˜ ì´ë¦„ í˜¹ì€ ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ ë¶„ê¸°ë¥¼ ë‚˜ëˆ„ì–´ ê°œì¸ ì‘ì—…ì„ ì§„í–‰í•˜ë©´ì„œ, ì‚¬ì „ì— ë…¼ì˜í–ˆë˜ ê²ƒì²˜ëŸ¼ scoreê°€ ì˜¬ëì„ ê²½ìš°ì—ë§Œ baseline ì½”ë“œì— PR-Mergeë¥¼ í•˜ê³ , í•´ë‹¹ scoreë¥¼ ì¬í˜„ê°€ëŠ¥í•  ìˆ˜ ìˆê²Œë” ë²„ì „ì—…í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.
</span>

![](/images/bran.png)

branchê°€ ë§ì•„ì§€ê¸´ í–ˆì§€ë§Œ, ì‹¤í—˜ì— ì‹¤íŒ¨í–ˆì„ ë•Œ ë¹ ë¥´ê²Œ Rollbackí•  ìˆ˜ ìˆì—ˆê³ , ê°ìì˜ ì‹¤í—˜ì—ì„œ í™•ì‹¤í•œ ì„±ëŠ¥í–¥ìƒ ìš”ì†Œë§Œì„ í•©ì¹  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. Competitionì´ë¼ëŠ” í”Œë«í¼ì˜ íŠ¹ì„±ìƒ 7ëª…ì˜ íŒ€ì›ì´ ê°ì ì‘ì„±í•œ ëª¨ë“  ì½”ë“œë“¤ì„ Reviewí•˜ê³  í•©ì¹˜ë©´ì„œ ì‘ì—…ì„ ì´ì–´ë‚˜ê°€ê¸°ì—ëŠ” ë§ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì„ í•„ìš”ë¡œ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê¸°ì¤€ì„ ë‘ê³  í•„ìš”í•  ë•Œë§Œ ì½”ë“œë¥¼ ë³‘í•©í•˜ë‹ˆ ì‹¤í—˜ì€ ì‹¤í—˜ëŒ€ë¡œ ì˜ ì´ë£¨ì–´ì§€ê³ , ì‹¤í—˜ì— ì‹¤íŒ¨í•˜ë”ë¼ë„ ê°€ì¥ ìµœì‹ ë²„ì „ì˜ ì½”ë“œë¥¼ ëª¨ë‘ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆì—ˆë‹¤ëŠ” ì ì—ì„œ ë§ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

### ğŸ§‘â€ğŸ”¬ ì²´ê³„ì ì¸ ì‹¤í—˜

![](/images/notion_kanban.png)

ì €í¬ëŠ” Kanban boardë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ê´€ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

<p align="center">
    <img src="/images/exp_tag.png" height="30%" width="30%">
</p>

Backlog, TO-DO, In progress, Completed ë„¤ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì„œë¡œê°€ ì–´ë–¤ ì‹¤í—˜ì„ ì§„í–‰í•˜ê³  ìˆëŠ”ì§€, ì–´ë–¤ ì‹¤í—˜ì„ í•´ì•¼í•˜ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ìœ¼ë©° ì‹¤í—˜ì´ ëë‚  ë•Œë§ˆë‹¤ ê·¸ë•Œê·¸ë•Œ ê°±ì‹ í•˜ëŠ” ì‘ì—…ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

ê°ê°ì˜ ì¹´í…Œê³ ë¦¬ ë³„ ì±…ì„ì€ ì´ë ‡ìŠµë‹ˆë‹¤.
- Backlog: ë‹¨ìˆœ ì‹¤í—˜ ì•„ì´ë””ì–´ ë° ê±´ì˜ì‚¬í•­, ìˆ˜ì •ì‚¬í•­
- TO-DO: ê¼­ ì ìš©í•´ë´ì•¼ í•˜ëŠ” ì‹¤í—˜
- In progress: í˜„ì¬ ì§„í–‰ì¤‘ì¸ ì‹¤í—˜
- Completed: ì™„ë£Œëœ ì‹¤í—˜

Backlogì— ì§„í–‰í•´ë³´ê³  ì‹¶ì€ ì‹¤í—˜ì¹´ë“œê°€ ìƒê²¼ê±°ë‚˜, ë‹¤ë¥¸ ì‹¤í—˜ ì•„ì´ë””ì–´ê°€ ìƒê¸´ ê²½ìš°ì—ëŠ” Notionì˜ `Comment` ê¸°ëŠ¥ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ ì§„í–‰ì¤‘ì¸ ì‹¤í—˜ì´ë©´ í•´ë‹¹ ì‹¤í—˜ì˜ ì§„í–‰ìƒí™©ì´ë‚˜ ì£¼ì˜ì‚¬í•­ë“¤ì„ ë” ìì„¸í•˜ê²Œ ê³µìœ í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.

<p align="center">
    <img src="/images/notion_comm2.png">
    <img src="/images/notion_comm.png">
</p>

ì‹¤í—˜ì¹´ë“œê°€ `Completed`ë¡œ ì´ë™í•˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì‹¤í—˜ê¸°ë¡í‘œì— ê²°ê³¼ë¥¼ ì‘ì„±í•˜ê³ , ì‹¤í—˜ì˜ ì„±ê³µì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ê·¸ ì‹¤í—˜ì— ëŒ€í•œ í‰ê°€ì™€ ê·¸ëŸ° ê²°ê³¼ê°€ ë‚˜ì˜¨ ì´ìœ  í˜¹ì€ ì£¼ì˜ì‚¬í•­ ë“±ì„ ê¸°ë¡í•˜ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.

<p align="center">
    <img src="/images/exp1.png">
    <img src="/images/notion_exp.png">
</p>

<span class="linear_highlight">
ì´ëŸ¬í•œ ì‹œë„ëŠ” íŒ€ì›ë“¤ ê°„ ê¸°ìˆ ë¶€ì±„ë¥¼ ìµœëŒ€í•œ ì¤„ì–´ë“¤ê²Œ í•˜ì˜€ê³  ì‹¤íŒ¨í•œ ì‹¤í—˜ì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ì§€ ì•Šì„ ìˆ˜ ìˆê²Œ í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í—˜ì„ ê³„íší•  ìˆ˜ ìˆê²Œ í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.
</span>

## ğŸ›‹ Data Experiments

### ğŸ‘ Data EDA

![](/images/eda.png)

ë°ì´í„°ëŠ” ìœ„ì™€ ê°™ì´ ë§¤ìš° ë¶ˆê· í˜•í•˜ê²Œ ë¶„í¬ë˜ì–´ ìˆì—ˆê³ , 9,000ê°œê°€ ë„˜ëŠ” no_relationê³¼ ë‹¬ë¦¬ `per:place_of_death`ì²˜ëŸ¼ ì•½ 40ê°œ ì •ë„ë§Œ ì¡´ì¬í•˜ëŠ” labelë„ ìˆì—ˆìŠµë‹ˆë‹¤.

ì´ë ‡ê²Œ ê·¹ë‹¨ì ì¸ Data Imbalancingì„ ì˜ ì¡ëŠ” ê²ƒì´ ì´ë²ˆ ëŒ€íšŒì˜ í•µì‹¬ì´ë¼ê³  ìƒê°í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

![ê¹€ì±„ì€ ìº í¼ë‹˜ì˜ í† ë¡ ê²Œì‹œíŒ ê¸€ ì¤‘](/images/dup.png)

ë˜í•œ sentenceì™€ subject_entity, object_entityê¹Œì§€ ì „ë¶€ ë™ì¼í•œ ë¬¸ì¥ì´ 53ê°œê°€ ìˆëŠ” ë“± ì¤‘ë³µëœ ë°ì´í„°ì™€ mislabeled ë°ì´í„°ë“¤ì´ ì¡´ì¬í•˜ì˜€ê³ , ì´ê²ƒë“¤ì„ ì „ë¶€ ì œê±°í•˜ê³  ìˆ˜ì •í•˜ì—¬ ë°ì´í„°ì…‹ì„ ì¬êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

### ğŸ’ª Data Augmentation

ì´í›„ì—ëŠ” Data Imbalancingì„ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ ì—¬ëŸ¬ Augmentation ê¸°ë²•ë“¤ë¡œ ì‹¤í—˜ì„ ì´ì–´ë‚˜ê°”ìŠµë‹ˆë‹¤.

1. [EDA & AEDA](https://github.com/toriving/KoEDA)
    KoEDA ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ EDA, AEDA ê°ê° ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ `n_aug=[1, 2, 4]` ë¹„ìœ¨ë¡œ augmentation ì§„í–‰
    - ë‘ ë°©ë²• ëª¨ë‘ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šì•˜ì„ ë•Œë³´ë‹¤ validation scoreê°€ ë‚®ì•˜ìŒ.
2. [Undersampling & Oversampling](https://imbalanced-learn.org/stable/)
    imblearn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ SMOTEë¡œ Sampling ì§„í–‰
    ![Undersampling](/images/under.png)

    ![Oversampling](/images/over.png)
    
    - ë‘ ë°©ë²• ëª¨ë‘ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šì•˜ì„ ë•Œë³´ë‹¤ validation scoreê°€ ë‚®ì•˜ìŒ.
3. Back Translation
    Crawlerë¥¼ ì‚¬ìš©í•˜ì—¬ papago ë²ˆì—­ê¸° ì‚¬ìš©.
    `klue/roberta-small` ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ score ìƒìŠ¹ì´ ìˆì—ˆì§€ë§Œ ë„ˆë¬´ ëŠ¦ê²Œ ì‹œë„í•´ì„œ best ëª¨ë¸ì— ì ìš©í•˜ì§€ ëª»í–ˆìŒ.
    - `ko -> en -> ja -> ko`: ì•½ 0.1 LB Score í•˜ë½
    - `ko -> ja -> ko`: ì•½ 0.5 LB Score ìƒìŠ¹
4. Target Augmentation subject <-> object label changing
    kfoldë¡œ í•™ìŠµì„ ì§„í–‰í•  ë•Œ í•œ ë²ˆì´ë¼ë„ í‹€ë¦° dataì— ëŒ€í•´ì„œ subjectì™€ object entityë¥¼ ë³€ê²½í•¨ìœ¼ë¡œì¨ augmentation ì§„í–‰
    - ì•½ 0.05 LB Score ìƒìŠ¹

í•˜ì§€ë§Œ ì´ëŸ¬í•œ augmentation ê¸°ë²•ë“¤ì— ëŒ€í•´ì„œ ë§ì€ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ê°€ ì—†ì—ˆëŠ”ë°, `confusion matrix`ë¥¼ í†µí•´ ì›ì¸ì„ ìœ ì¶”í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

![](/images/conf.png)

ì²˜ìŒ ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ ì ì€ labelì˜ ë°ì´í„°ë¥¼ ìƒê°ë³´ë‹¤ ì˜ ë§ì¶”ê³  ìˆì—ˆê³ , *ì˜¤íˆë ¤ ë°ì´í„° ìˆ˜ê°€ ê°€ì¥ ë§ì•˜ë˜ `no_relation` ì˜ˆì¸¡ì—ì„œ ë§ì´ í‹€ë¦¬ê³  ìˆì—ˆê¸° ë•Œë¬¸* ì´ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ augmentationì„ ì§„í–‰í•œ ë°©ì‹, ê·¸ë¦¬ê³  sampling ë°©ì‹ìœ¼ë¡œëŠ” íš¨ê³¼ë¥¼ ë³´ì§€ ëª»í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ì²˜ìŒë¶€í„° ì´ë ‡ê²Œ Confusion Matrixë¥¼ ë„ì…í•˜ì—¬ í˜„ì¬ ëª¨ë¸ì´ ì–´ë–¤ ì˜ˆì¸¡ì„ ì˜ ìˆ˜í–‰í•˜ì§€ ëª»í•˜ëŠ”ì§€ ë“±ì„ íŒŒì•…í•˜ì—¬, `no_relation`ì— ëŒ€í•´ì„œë§Œ augmentationì„ ì‹œë„í•˜ëŠ” ë“± ë””í…Œì¼í•˜ê²Œ augmentation ì „ëµì„ ì„¸ì› ìœ¼ë©´ ì¢‹ì•˜ì„ ê²ƒì´ë€ ì•„ì‰¬ì›€ì´ ë‚¨ìŠµë‹ˆë‹¤. ë˜í•œ 4ë²ˆ ì‹¤í—˜ì—ì„œ `sub <-> obj` labelë§Œ ë³€ê²½í•˜ëŠ” ë°©ì‹ ë§ê³  ë‹¤ë¥¸ augmentation ë°©ë²•ë„ ì¨ë´¤ìœ¼ë©´ ì–´ë• ì„ê¹Œ í•˜ëŠ” ì•„ì‰¬ì›€ì´ ë‚¨ìŠµë‹ˆë‹¤.


### ğŸ”§ Data Preprocessing & Tokenizer

1. Dynamic Padding
    Huggingfaceì˜ TokenizerëŠ” `max_length` ì¸ìë¥¼ í†µí•´ ê¸°ë³¸ì ìœ¼ë¡œ `fixed padding` ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì €í¬ëŠ” ë” ë¹ ë¥¸ ì‹¤í—˜ì„ í†µí•´ `dynamic padding` ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ì˜€ê³  ê·¸ ê²°ê³¼ ì•½ 30%ì˜ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

    ![fixed padding](/images/fixed.png)

    ![dynamic padding](/images/dynamic.png)

2. [An Improved Baseline for Sentence-level Relation Extraction](https://arxiv.org/pdf/2102.01373.pdf)
    ë¬¸ì¥ì˜ Subject, Object Entityì˜ NER Typeì„ ëª…ì‹œí•´ì£¼ê³ , Entityì˜ ìœ„ì¹˜ë¥¼ ì‚¬ì „í•™ìŠµì—ì„œ ì‚¬ìš©ëœ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì´ìš©í•˜ì—¬ í‘œê¸°í•˜ëŠ” Typed Entity Markerë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.

    ![](/images/token.png)
    1. vanilla : ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ input
    2. special_ent : `ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ input + [sbj][sbj/] + [obj][obj/]`
    3. special_ent_without_prefix : ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ inputì˜ ì•ì—ìˆëŠ” `subject [sep] object [sep] ë¶€ë¶„ì„ ì œê±°`í•˜ê³  `special token`ì„ ì‚¬ìš© (4, 5ë²ˆ ì—­ì‹œ prefixë¥¼ ì œê±°í•¨) 
    4. punct_ent : `@sbj@ #obj#` ì‹ìœ¼ë¡œ special token ì—†ì´ entity í‘œí˜„ 
    5. punct_typing_ent : `@*sbj_type*sbj@ #^obj_type^obj#` ì‹ìœ¼ë¡œ entity typeì„ ì•Œë ¤ì£¼ë©° í‘œí˜„ê²°ê³¼: 3ë²ˆ 5ë²ˆì´ ë¹„êµì  ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„, ë™ì¼ ì¡°ê±´ í•˜ validation f1 ê¸°ì¤€ 1 ì •ë„ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì„.

ìœ„ì˜ ë‘ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ê³¼ ê²€ì¦ì€ ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ ì§„í–‰í•  ìˆ˜ ìˆì—ˆê³ , ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.


## ğŸ§˜ Modeling

Backboneì´ ë˜ëŠ” Modelì€ `klue/roberta-large`ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©° Base ì„±ëŠ¥ì€ `avg. 71 (micro f1)` ì •ë„ë¥¼ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.

***ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ë“¤***

- Entity Embedding
    ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šì•˜ì„ ë•Œë³´ë‹¤ validation scoreê°€ ë‚®ì•˜ìŒ.
<details>
    <summary>
    ì½”ë“œ ë³´ê¸°
    </summary>
    ```python
    class RobertaEmbeddingsWithTokenEmbedding(nn.Module):
    '''
    edit by ê³½ì§„ì„±_T2011
    '''
    
    def __init__(self, model, config, pre_model_state_dict=None):
        super().__init__()
        self.word_embeddings = model.roberta.embeddings.word_embeddings
        self.position_embeddings = model.roberta.embeddings.position_embeddings
        self.token_type_embeddings = model.roberta.embeddings.token_type_embeddings

        self.entity_embeddings = nn.Embedding(9, config.hidden_size, padding_idx=0)

        if pre_model_state_dict:
            pre_weight = pre_model_state_dict['roberta.embeddings.entity_embeddings.weight']
            self.entity_embeddings.weight = torch.nn.parameter.Parameter(pre_weight, requires_grad=True)

        self.LayerNorm = model.roberta.embeddings.LayerNorm
        self.dropout = model.roberta.embeddings.dropout
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)))
        if version.parse(torch.__version__) > version.parse("1.6.0"):
            self.register_buffer(
                "token_type_ids",
                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),
                persistent=False,
            )
        self.padding_idx = config.pad_token_id

    def forward(
        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0
    ):
        if position_ids is None:
            if input_ids is not None:
                position_ids = self.create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)
            else:
                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)

        if input_ids is not None:
            input_shape = input_ids.size()
        else:
            input_shape = inputs_embeds.size()[:-1]

        seq_length = input_shape[1]

        if token_type_ids is None:
            if hasattr(self, "token_type_ids"):
                buffered_token_type_ids = self.token_type_ids[:, :seq_length]
                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)
                token_type_ids = buffered_token_type_ids_expanded
            else:
                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)

        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        entity_ids = self.create_entity_ids_from_input_ids(input_ids)
        entity_embeddings = self.entity_embeddings(entity_ids)

        embeddings = inputs_embeds + token_type_embeddings
        if self.position_embedding_type == "absolute":
            position_embeddings = self.position_embeddings(position_ids)
            embeddings += position_embeddings
        
        embeddings += entity_embeddings
        
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

    def create_position_ids_from_inputs_embeds(self, inputs_embeds):
        input_shape = inputs_embeds.size()[:-1]
        sequence_length = input_shape[1]

        position_ids = torch.arange(
            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device
        )
        return position_ids.unsqueeze(0).expand(input_shape)

    def create_entity_ids_from_input_ids(self, input_ids):
        """
        map index 1~8 to the token that is related to sbj, obj entities
        """
        s_ids = torch.nonzero((input_ids == 36)) # subject
        o_ids = torch.nonzero((input_ids == 7)) # object
        # entity type mapped into index 3 ~ 8
        type_map = {4410 : 3, 7119 : 4, 3860 : 5, 5867 : 6, 12395 : 7, 9384 : 8}

        entity_ids = torch.zeros_like(input_ids)
        for i in range(len(s_ids)):
            s_id = s_ids[i]
            o_id = o_ids[i]
            if i % 2 == 0:
                entity_ids[s_id[0], s_id[1]+2] = type_map[input_ids[s_id[0], s_id[1]+2].item()]
                entity_ids[o_id[0], o_id[1]+2] = type_map[input_ids[o_id[0], o_id[1]+2].item()]
            else:
                prev_s_id = s_ids[i-1]
                prev_o_id = o_ids[i-1]
                entity_ids[s_id[0], prev_s_id[1]+4:s_id[1]] = 1
                entity_ids[o_id[0], prev_o_id[1]+4:o_id[1]] = 2

        return entity_ids

    def create_position_ids_from_input_ids(self, input_ids, padding_idx, past_key_values_length=0):
        mask = input_ids.ne(padding_idx).int()
        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask
        return incremental_indices.long() + padding_idx
    ```
</details>
- [R-BERT](https://github.com/monologg/R-BERT)
    ë³¸ êµ¬ì¡°ì—ì„œ BERTë¥¼ RoBERTaë¡œ ë³€ê²½. LB ê¸°ì¤€ 71.362ì˜ micro f1 score ë‹¬ì„±.
<details>
    <summary>
    ì½”ë“œ ë³´ê¸°
    </summary>
    ```python
class RBERT(RobertaPreTrainedModel):
    '''
    orgin code: https://github.com/monologg/R-BERT
    edit by ë¬¸í•˜ê²¸_T2076
    '''

    def __init__(self, config, model_name):
        super(RBERT, self).__init__(config)
        self.roberta = RobertaModel.from_pretrained(
            model_name)  # Load pretrained bert

        self.num_labels = config.num_labels

        self.cls_fc_layer = FCLayer(
            config.hidden_size, config.hidden_size, 0.1)
        self.entity_fc_layer = FCLayer(
            config.hidden_size, config.hidden_size, 0.1)
        self.label_classifier = FCLayer(
            config.hidden_size * 3,
            config.num_labels,
            0.1,
            use_activation=False,
        )

    @staticmethod
    def entity_average(hidden_output, e_mask):
        """
        Average the entity hidden state vectors (H_i ~ H_j)
        :param hidden_output: [batch_size, j-i+1, dim]
        :param e_mask: [batch_size, max_seq_len]
                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]
        :return: [batch_size, dim]
        """
        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]
        length_tensor = (e_mask != 0).sum(
            dim=1).unsqueeze(1)  # [batch_size, 1]

        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]
        sum_vector = torch.bmm(e_mask_unsqueeze.float(),
                               hidden_output).squeeze(1)
        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting
        return avg_vector

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, e1_mask=None, e2_mask=None):
        outputs = self.roberta(
            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        sequence_output = outputs[0]
        pooled_output = outputs[1]

        # Average
        e1_h = self.entity_average(sequence_output, e1_mask)
        e2_h = self.entity_average(sequence_output, e2_mask)

        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)
        pooled_output = self.cls_fc_layer(pooled_output)
        e1_h = self.entity_fc_layer(e1_h)
        e2_h = self.entity_fc_layer(e2_h)

        # Concat -> fc_layer
        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)
        logits = self.label_classifier(concat_h)

        # add hidden states and attention if they are here
        outputs = (logits,) + outputs[2:]

        # Softmax
        if labels is not None:
            if self.num_labels == 1:
                loss_fct = nn.MSELoss()
                loss = loss_fct(logits.view(-1), labels.view(-1))
            else:
                loss_type = "focal"
                beta = 0.9999
                gamma = 2.0

                loss_fct = CB_loss(beta=beta, gamma=gamma)
                loss = loss_fct(logits.view(-1, self.num_labels),
                                labels.view(-1), loss_type)

            outputs = (loss,) + outputs
        return outputs
    ```
</details>

- Model split & combine
    `no_relation`ë§Œ êµ¬ë¶„í•˜ë„ë¡ í•™ìŠµì‹œí‚¨ ëª¨ë¸, `relation`ë§Œ êµ¬ë¶„í•˜ë„ë¡ í•™ìŠµì‹œí‚¨ ëª¨ë¸, `ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸` ì„¸ ê°€ì§€ `klue/roberta-large` ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ freezing í•˜ê³  classifierë§Œ í•™ìŠµì‹œí‚¨ ê²ƒ. LB ê¸°ì¤€ 73.251ì˜ micro f1 score ë‹¬ì„±.

    <details>
        <summary>
        ì½”ë“œ ë³´ê¸°
        </summary>
        ```python
    class CombineModels(nn.Module):
        '''
        edit by ì´ìš”í•œ_T2166
        '''
        def __init__(self):
            super(CombineModels, self).__init__()

            c1 = AutoConfig.from_pretrained('klue/roberta-large', num_labels=2)
            c2 = AutoConfig.from_pretrained('klue/roberta-large', num_labels=29)
            c3 = AutoConfig.from_pretrained('klue/roberta-large', num_labels=30)

            self.roberta1 = AutoModelForSequenceClassification.from_pretrained(
                "split_model_no_rel_large", config=c1)
            self.roberta2 = AutoModelForSequenceClassification.from_pretrained(
                "split_model_rel_large", config=c2)
            self.roberta3 = AutoModelForSequenceClassification.from_pretrained(
                "sota_kfold", config=c3)

            for p in self.roberta1.parameters():
                p.requires_grad = False
            for p in self.roberta2.parameters():
                p.requires_grad = False
            for p in self.roberta3.parameters():
                p.requires_grad = False

            self.fc1 = nn.Linear(2, 768)
            self.fc2 = nn.Linear(29, 768)
            self.fc3 = nn.Linear(30, 768)

            self.classifier = nn.Sequential(
                nn.Dropout(p=0.1),
                nn.Linear(768 * 15, 768, bias=True),
                nn.Tanh(),
                nn.Dropout(p=0.1),
                nn.Linear(768, 30, bias=True)
            )

        def forward(self, input_ids, attention_mask):
            logits_1 = self.roberta1(
                input_ids.clone(), attention_mask=attention_mask).get('logits')
            logits_2 = self.roberta2(
                input_ids.clone(), attention_mask=attention_mask).get('logits')
            logits_3 = self.roberta3(
                input_ids.clone(), attention_mask=attention_mask).get('logits')

            logits_1 = self.fc1(logits_1)
            logits_2 = self.fc2(logits_2)
            logits_3 = self.fc1(logits_3)

            concatenated_vectors = torch.cat((
                logits_1, logits_2, logits_3), dim=-1)

            output = self.classifier(concatenated_vectors)
            outputs = SequenceClassifierOutput(logits=output)
            return outputs
        ```
    </details>
- FC Layer -> LSTM
    ë„ˆë¬´ ëŠ¦ê²Œ ë„ì…í•˜ì—¬ ì œì¶œ ì‹¤íŒ¨.
<details>
    <summary>
    ì½”ë“œ ë³´ê¸°
    </summary>
    ```python
    class RobertaAddLSTM(RobertaPreTrainedModel):
        '''
        edit by ì •í¬ì˜_T2210
        '''
        def __init__(self, config, *args, **kwargs):
                super().__init__(config=config)

                self.bert = RobertaModel.from_pretrained("klue/roberta-large")

                self.lstm = nn.LSTM(1024, 256, batch_first=True, bidirectional=True)
                self.linear = nn.Linear(256*2, 30)
                self.dropout = nn.Dropout(0.5)
                self.tanh = nn.Tanh()
                self.linear2 = nn.Linear(30, 1)

        def forward(self, input_ids, attention_mask):
                output = self.bert(input_ids, attention_mask=attention_mask)

                lstm_output, (h,c) = self.lstm(output[0]) ## extract the 1st token's embeddings
                hidden = torch.cat((lstm_output[:,-1, :256],lstm_output[:,0, 256:]),dim=-1)
                linear_output = self.linear(hidden.view(-1,256*2))
                x = self.tanh(linear_output)
                x = self.dropout(x)
                outputs = SequenceClassifierOutput(logits=x)

                return outputs
    ```
</details>
- TAPT - [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/pdf/2004.10964.pdf)
    ì£¼ì–´ì§„ í•™ìŠµë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì— TAPT ë¥¼ ì ìš©í•´ë³´ì•˜ì„ ë•Œ ì•½ 0.5 ì •ë„ì˜ validation f1 score í–¥ìƒì´ ìˆì—ˆìœ¼ë‚˜, ì‹œê°„ë¬¸ì œë¡œ ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ epochsë§Œí¼ í•™ìŠµì„ ì§„í–‰í•˜ì§€ ëª»í–ˆìŒ. koelectraì™€ roberta-baseë¡œ ë¦¬ë”ë³´ë“œì— ì œì¶œí•´ë³¸ ê²°ê³¼ í° ì„±ëŠ¥í–¥ìƒì´ ì—†ì—ˆê¸° ë•Œë¬¸ì— large ëª¨ë¸ì— ì ìš©í•´ë³¼ ìˆ˜ ì—†ì—ˆìŒ.

    <details>
        <summary>
        ì½”ë“œ ë³´ê¸°
        </summary>
        ```python
        '''
        edit by ì •ì§„ì›_T2206
        '''
        from transformers import AutoTokenizer, RobertaForMaskedLM, ElectraForMaskedLM, BertForMaskedLM, AutoConfig, DataCollatorWithPadding, DataCollatorForLanguageModeling
        import torch
        from transformers import LineByLineTextDataset
        from transformers import Trainer, TrainingArguments
        from transformers import EarlyStoppingCallback

        # fetch pretrained model for MaskedLM training 
        tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = BertForMaskedLM.from_pretrained('klue/roberta-large')
        model.to(device)

        # Read txt file which is consisted of sentences from train.csv
        dataset = LineByLineTextDataset(
            tokenizer=tokenizer,
            file_path='data/train.txt',
            block_size=514 # block size needs to be modified to max_position_embeddings
        )

        data_collator = DataCollatorForLanguageModeling( 
            tokenizer=tokenizer, mlm=True, mlm_probability=0.2 
        )

        # need to change arguments 
        training_args = TrainingArguments(
            output_dir="./klue-roberta-retrained",
            overwrite_output_dir=True,
            learning_rate=5e-05,
            num_train_epochs=200, 
            per_device_train_batch_size=16,
            save_steps=100,
            save_total_limit=2,
            seed=30,
            save_strategy='epoch',
            gradient_accumulation_steps=8,
            logging_steps=100,
            evaluation_strategy='epoch',
            resume_from_checkpoint=True,
            fp16=True,
            fp16_opt_level='O1',
            load_best_model_at_end=True
        ) 

        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset,
            eval_dataset=dataset,
            callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]
        )

        trainer.train()
        trainer.save_model("./klue-roberta-retrained")
        ```
    </details>


***loss function***
- CB Loss
    R-BERTë¥¼ ì œì™¸í•˜ê³ ëŠ” í° ì„±ëŠ¥í–¥ìƒì„ ëª»ë´¤ìŒ.

    <details>
        <summary>
        ì½”ë“œ ë³´ê¸°
        </summary>
        ```python
        class MyTrainer(Trainer):
            '''
            edit by ë¬¸í•˜ê²¸_T2076
            '''
            def __init__(self, disable_wandb=True, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.disable_wandb = disable_wandb

            def compute_loss(self, model, inputs, return_outputs=False):
                labels = inputs.get("labels")
                outputs = model(**inputs)
                logits = outputs.get("logits")
                beta = 0.9999
                gamma = 2.0

                criterion = CB_loss(beta, gamma)
                if torch.cuda.is_available():
                    criterion.cuda()
                loss_fct = criterion(logits, labels)

                return (loss_fct, outputs) if return_outputs else loss_fct

            def evaluation_loop(self, *args, **kwargs):
                eval_loop_output = super().evaluation_loop(*args, **kwargs)

                pred = eval_loop_output.predictions
                label_ids = eval_loop_output.label_ids

                self.draw_confusion_matrix(pred, label_ids)
                return eval_loop_output

        class CB_loss(nn.Module):
            def __init__(self, beta, gamma, epsilon=0.1):
                super(CB_loss, self).__init__()
                self.beta = beta
                self.gamma = gamma
                self.epsilon = epsilon

            def forward(self, logits, labels):
                # self.epsilon = 0.1 #labelsmooth
                beta = self.beta
                gamma = self.gamma

                no_of_classes = logits.shape[1]
                samples_per_cls = torch.Tensor(
                    [sum(labels == i) for i in range(logits.shape[1])])
                if torch.cuda.is_available():
                    samples_per_cls = samples_per_cls.cuda()

                effective_num = 1.0 - torch.pow(beta, samples_per_cls)
                weights = (1.0 - beta) / ((effective_num) + 1e-8)

                weights = weights / torch.sum(weights) * no_of_classes
                labels = labels.reshape(-1, 1)

                weights = torch.tensor(weights.clone().detach()).float()

                if torch.cuda.is_available():
                    weights = weights.cuda()
                    labels_one_hot = torch.zeros(
                        len(labels), no_of_classes).cuda().scatter_(1, labels, 1).cuda()

                labels_one_hot = (1 - self.epsilon) * labels_one_hot + \
                    self.epsilon / no_of_classes
                weights = weights.unsqueeze(0)
                weights = weights.repeat(labels_one_hot.shape[0], 1) * labels_one_hot
                weights = weights.sum(1)
                weights = weights.unsqueeze(1)
                weights = weights.repeat(1, no_of_classes)

                cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)
                return cb_loss
        ```
    </details>
- [LDAM](https://arxiv.org/pdf/1906.07413.pdf)
    <details>
        <summary>
        ì½”ë“œ ë³´ê¸°
        </summary>
        ```python
        class LDAMLossTrainer(Trainer):
            '''
            edit by ê¹€ë¯¼ìˆ˜_T2025
            '''
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.n_per_labels = self.train_dataset.get_n_per_labels()

            def compute_loss(self, model, inputs, return_outputs=False):
                labels = inputs.get('labels')
                outputs = model(**inputs)
                logits = outputs.get('logits')

                betas = [0, 0.99]
                beta_idx = self.state.epoch >= 2
                n_per_labels = self.n_per_labels

                effective_num = 1.0 - np.power(betas[beta_idx], n_per_labels)
                cls_weights = (1.0 - betas[beta_idx]) / np.array(effective_num)
                cls_weights = cls_weights / np.sum(cls_weights) * len(n_per_labels)
                cls_weights = torch.FloatTensor(cls_weights)

                criterion = LDAMLoss(cls_num_list=n_per_labels, max_m=0.5, s=30, weight=cls_weights)
                if torch.cuda.is_available():
                    criterion.cuda()

                loss_fct = criterion(logits, labels)
                return (loss_fct, outputs) if return_outputs else loss_fct

        class LDAMLoss(nn.Module):
            def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):
                super().__init__()
                m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))
                m_list = m_list * (max_m / np.max(m_list))
                m_list = torch.cuda.FloatTensor(m_list)
                self.m_list = m_list
                assert s > 0
                self.s = s
                self.weight = weight

            def forward(self, x, target):
                index = torch.zeros_like(x, dtype=torch.bool)
                index.scatter_(1, target.data.view(-1, 1), 1)

                index_float = index.type(torch.cuda.FloatTensor)
                batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0, 1))
                batch_m = batch_m.view((-1, 1))
                x_m = x - batch_m

                output = torch.where(index, x_m, x)
                return F.cross_entropy(self.s * output.to('cuda'), target.to('cuda'), weight=self.weight.to('cuda'))
        ```
    </details>

ê²°ê³¼ì ìœ¼ë¡  backbone ëª¨ë¸ì˜ kfold scoreë¥¼ ë„˜ì–´ì„œì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ë¡œëŠ” public leaderboardì™€ ê°„ê·¹ì´ ì ì€ ì ì ˆí•œ validation datasetì„ êµ¬ì„±í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 

ì°¨ì„ ì±…ìœ¼ë¡œ ë¯¸ë¦¬ stratifiedí•˜ê²Œ 0.1 ë¹„ìœ¨ë¡œ splití•œ train, validation datasetì„ ê³ ì •ì‹œì¼œë†“ê³  ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, í•´ë‹¹ ë°ì´í„°ì…‹ì˜ validation scoreëŠ” public leaderboardì™€ ***í‰ê· ì ìœ¼ë¡œ 15ì  ì´ìƒ, ê·¹ë‹¨ì ì¸ ê²½ìš° 30ì ê¹Œì§€ ì°¨ì´ê°€ ì¡´ì¬*** í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ validation datasetìœ¼ë¡œë§Œ ì‹¤í—˜ì„ í–ˆê¸° ë•Œë¬¸ì— validation scoreë¥¼ ì‹ ë¢°í•˜ê¸° ì–´ë ¤ì› ê³ , modelì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ ì§ì ‘ ì œì¶œí•´ë³´ê¸° ì „ì—ëŠ” ì•Œ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.

ë”°ë¼ì„œ ì œì¶œíšŸìˆ˜ê°€ í•œì •ì ì´ë¯€ë¡œ ëŒ€ë¶€ë¶„ ìœ„ì˜ validation scoreë¡œë§Œ ê²€ì¦ì„ í–ˆê³ , ì„±ëŠ¥í–¥ìƒ ê°€ëŠ¥ì„±ì´ ì—†ë‹¤ê³  íŒë‹¨í•´ ì¶”ê°€ì ì¸ ì‹¤í—˜ì„ ì§„í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë§‰ìƒ 1, 2ë“±ì˜ solutionì„ ë³´ë‹ˆ ì €í¬ê°€ ì§„í–‰í–ˆë˜ ì‹¤í—˜ë“¤ë¡œ ì ìˆ˜ë¥¼ ì˜¬ë ¸ê¸°ì— ì†ì´ ì“°ë ¸ìŠµë‹ˆë‹¤... :(

ë‹¤ìŒ ëŒ€íšŒì—ì„œëŠ” public leaderboardì™€ì˜ ê°„ê·¹ì´ ì ì€ ì ì ˆí•œ validation datasetì„ êµ¬ì„±í•  í•„ìš”ê°€ ìˆìœ¼ë©°, ë˜í•œ êµì°¨ê²€ì¦ì„ ìœ„í•œ validation dataset ì—­ì‹œ ê³ ì •ì‹œì¼œë†“ì„ í•„ìš”ê°€ ìˆë‹¤ê³  ëŠê¼ˆìŠµë‹ˆë‹¤.


## ğŸœ Ensemble

- K-fold

    `klue/roberta-large` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ kfold(k=5)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤. single fold ê¸°ì¤€ public leaderboardì—ì„œ ì•½ 71ì˜ micro f1 scoreë¥¼ ê¸°ë¡í•˜ì˜€ê³ , 5 fold ensembleì„ í†µí•´ public leaderboardì—ì„œ 73.5ì˜ micro f1 scoreë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

### ğŸ‘¨â€ğŸ¨ ì•™ìƒë¸” ê¹ëŠ” ë…¸ì¸ê³¼ ê¸°ë„ë©”íƒ€

![soft voting](/images/soft_voting.jpg)

ì €í¬ëŠ” public leaderboard ê¸°ì¤€ 73 ì •ë„ì˜ micro f1 scoreë¥¼ ê¸°ë¡í•˜ì˜€ì§€ë§Œ ì˜ˆì¸¡ ë¶„í¬ê°€ ë‹¤ë¥¸ ê²°ê³¼ë“¤ì„ `soft voting`í•˜ì—¬ public leaderboard ê¸°ì¤€ 74.306ì˜ micro f1 scoreë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

<p align="center">
    <img src="/images/ê¸°ë„1.png" style="display: inline" height="100px">
    <img src="/images/ê¸°ë„2.png" style="display: inline" height="100px">
    <img src="/images/ê¸°ë„3.png" style="display: inline" height="100px">
    <img src="/images/ê¸°ë„4.png" style="display: inline" height="100px">
</p>
<p align="center">
    <img src="/images/ê¸°ë„5.png" height="100px">
</p>

<p>
    ìµœì¢…ì ìœ¼ë¡œ ì œì¶œëœ ê²°ê³¼ëŠ” Ensembleëœ ê²ƒë“¤ ì¤‘ public leaderboard ê¸°ì¤€ AUPRCê°€ ê°€ì¥ ë†’ì€ ê²°ê³¼ì´ë©°, private leaderboard ì—ì„œ ë‹¤ë¥¸ íŒ€ ëŒ€ë¹„ ì ìˆ˜ í•˜ë½í­ì´ ì ì–´ì„œ 
    <span class="linear_highlight">
    ìˆœìœ„ê°€ 9ë“± -> 5ë“±ìœ¼ë¡œ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤.
    </span>
</p>

ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§ ì‹¤í—˜ì—ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì§„ ëª»í–ˆì§€ë§Œ, ë¹„ìŠ·í•œ ì ìˆ˜ì˜ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ê²°ê³¼ë“¤ì„ ë§ì´ ë§Œë“¤ì–´ë†“ì•˜ë˜ ê²ƒì´ ì•™ìƒë¸”ì—ì„œ ë”ìš± ì¼ë°˜í™”ëœ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë˜ ìš”ì¸ì´ì—ˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.

![](/images/final.png)


## ğŸ’¯ Good Practice

ì €í¬ íŒ€ë§Œì˜ Good PracticeëŠ” ì²´ê³„ì ìœ¼ë¡œ Notionì— ì‹¤í—˜ê´€ë¦¬ë¥¼ í•œ ê²ƒë„ ìˆì§€ë§Œ, `huggingface`ì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ë“¤ì„ ì‚¬ìš©í•´ë´¤ë‹¤ëŠ” ê²ƒì´ê³  ê·¸ ì¤‘ì—ì„œ ì¢‹ì€ íš¨ê³¼ë¥¼ ë‚¸ ê²ƒìœ¼ë¡œëŠ” `fp16`ê³¼ `hyperparameter_search`ê°€ ìˆìŠµë‹ˆë‹¤.

- fp16

    ![](/images/fp16.png)
    fp16ì€ `Mixed-Precision Training`ìœ¼ë¡œ 32-bit Floating Pointê°€ ì•„ë‹Œ 16-bit Floating Pointë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ ì„±ëŠ¥ì€ ë¹„ìŠ·í•˜ì§€ë§Œ ì•½ 60% ê°€ëŸ‰ì˜ í–¥ìƒëœ ì†ë„ë¡œ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
    `TrainingArguments`ì— `fp16=True`, `fp16_opt_level='O1'`ë§Œ ì¶”ê°€í•˜ë©´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ì„œ ê°„ë‹¨í•˜ê²Œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.


- hyperparameter_search

    `hyperparameter_search`ëŠ” `Trainer`ì— ì¡´ì¬í•˜ëŠ” methodë¡œ `raytune`, `optuna`, `SigOpt` ì„¸ ê°€ì§€ ì¤‘ ìì‹ ì˜ í™˜ê²½ì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ì ì ˆí•œ hyperparameterë¥¼ íƒìƒ‰í•´ì£¼ëŠ” ìœ ìš©í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì €í¬ëŠ” `hyperparameter_search`ë¡œ public leaderboard ê¸°ì¤€ 4ì  ì´ìƒì˜ f1 score í–¥ìƒì„ ê¸°ë¡í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

    <details>
        <summary>
        ì½”ë“œ ë³´ê¸°
        </summary>
        ```python
        from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, Trainer, TrainingArguments

        # TODO: load your tokenizer & dataset
        # tokenizer = ...
        # dataset = ...

        # TODO: change your pretrained model path
        config = AutoConfig.from_pretrained("YOUR_MODEL_PATH")

        def model_init():
            return AutoModelForSequenceClassification.from_pretrained(
                model_path, config=config)

        # TODO: fill it your training arguments
        training_args = TrainingArguments(...)

        # TODO: fill it your trainer arguments
        trainer = Trainer(
            model_init=model_init, # NOTE: ë°˜ë“œì‹œ model_init í•¨ìˆ˜ë¡œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì•¼í•©ë‹ˆë‹¤.
            args=training_args,
            ...
        )

        # NOTE: optuna
        def optuna_hp_space(trial):
            return {
                "learning_rate": trial.suggest_float("learning_rate", 5e-6, 5e-4, log=True),
                "num_train_epochs": trial.suggest_int("num_train_epochs", 1, 5),
                "seed": trial.suggest_int("seed", 1, 42),
            }

        # NOTE: ray tune
        def ray_hp_space():
            from ray import tune
            return {
                "learning_rate": tune.loguniform(5e-6, 5e-4),
                "num_train_epochs": tune.choice(range(1, 6)),
                "seed": tune.choice(range(1, 42)),
            }

        trainer.hyperparameter_search(
            direction="maximize", # NOTE: or direction="minimize"
            hp_space=ray_hp_space, # NOTE: if you wanna use optuna, change it to optuna_hp_space
            backend="ray", # NOTE: if you wanna use optuna, remove this argument
        )
        ```
    </details>

    ![ë‹¨ì  - ìš©ëŸ‰ê½‰ì°¸](/images/ìš©ëŸ‰ê½‰ì°¸.png)

ë˜í•œ `W&B`ë¥¼ íŒ€ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ íŒ€ì›ë“¤ì´ ì‹¤í—˜í•˜ëŠ” ê²°ê³¼ë“¤ì„ ì „ë¶€ ê³µìœ í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë•ë¶„ì— ì‹¤íŒ¨í•œ ì‹¤í—˜ì´ë‚˜ ì„±ê³µí•œ ì‹¤í—˜ë“¤ì— ëŒ€í•´ì„œ chartë¥¼ í†µí•´ ë”ìš± ì‰½ê³  ì§ê´€ì ìœ¼ë¡œ ëª¨ë¸ì„ ê²€ì¦í•  ìˆ˜ ìˆì—ˆê³ , íŒ€ì›ê°„ ë” ë¹ ë¥¸ ê²°ê³¼ ê³µìœ ê°€ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤. 

<p align="center">
    <img src="/images/w0.png" height="300px">
</p>

<p align="center">
    <img src="/images/w1.png" style="display: inline" height="120px">
    <img src="/images/w2.png" style="display: inline" height="120px">
</p>
<p align="center">
    <img src="/images/w3.png" style="display: inline" height="120px">
    <img src="/images/w4.png" style="display: inline" height="120px">
</p>

## ğŸ’Œ Thanks to

***ì²­ê³„ì‚°ì…°ë¥´íŒŒì˜ ë¹„ë°€ë³‘ê¸°, ì´ìœ ê²½ ë©˜í† ë‹˜.***

<p align="center">
    <img src="/images/notion_mentoring.png" style="display: inline" height="360px">
    <img src="/images/notion_qna.png" style="display: inline" height="360px">
</p>

ì •ë§ ë°”ì˜ì‹  ì™€ì¤‘ì—ë„ ë§ì€ ê²ƒì„ ì•Œë ¤ì£¼ì‹œë ¤ê³  ì—´ì‹¬íˆ ì°¾ì•„ë³´ì‹œê³ , ë”°ë¡œ ê³µë¶€ë„ í•´ê°€ì‹œë©´ì„œ ì €í¬ì—ê²Œ ë§ì€ ë„ì›€ì„ ì£¼ì…¨ìŠµë‹ˆë‹¤. ì €í¬ì˜ ë“±ë°˜ì¼ì§€ì— ê°€ì¥ í° ê¸°ì—¬ë¥¼ í•˜ì‹  ì´ìœ ê²½ ë©˜í† ë‹˜ê»˜ ë‹¤ì‹œí•œë²ˆ ê°ì‚¬ì˜ ë§ì”€ ì „í•´ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤.

<br>

***ì²­ê³„ì‚°ì…°ë¥´íŒŒ íŒ€ëª…ì— í•­ìƒ ë¶ˆë§Œì„ ê°€ì§€ì‹œëŠ” ì„±ì˜ˆë‹® ë©˜í† ë‹˜***
![](/images/1.png)

ìœ ê²½ë©˜í† ë‹˜ì˜ ì‚¬ìƒíŒ¬ë‹µê²Œ ì €í¬íŒ€ì—ë„ ë§ì€ ê´€ì‹¬ê°€ì ¸ì£¼ì‹œê³  ì§€ì¼œë´ì£¼ì…”ì„œ ì •ë§ ë“ ë“ í•©ë‹ˆë‹¤. í•­ìƒ ë§ì”€ëª»ë“œë¦¬ëŠ”ê²Œ ì£„ì†¡í• ì •ë„ë¡œ, ë§ì€ ë„ì›€ ì£¼ì‹œê³  ì•Œë ¤ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤.


## ë§ˆì§€ë§‰ìœ¼ë¡œ

ì €í¬ê°€ ì²˜ìŒì— ê³„íší–ˆë˜ `ê¸°ë¡`ê³¼ `ê³µìœ `ë¼ëŠ” ê°€ì¹˜ì— ìˆì–´ì„œë§Œí¼ì€ ì „ë°˜ì ìœ¼ë¡œ íŒ€ì› ëª¨ë‘ê°€ ë§Œì¡±í•  ìˆ˜ ìˆì—ˆë˜ í”„ë¡œì íŠ¸ì˜€ìŠµë‹ˆë‹¤.

ì²˜ìŒ í•©ì„ ë§ì¶¤ì—ë„ ë¶ˆêµ¬í•˜ê³  íŒ€ì› ëª¨ë‘ê°€ ë‹¤ìŒ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì–´ë– í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ê³ , ì–´ë–¤ ì‹ìœ¼ë¡œ í˜‘ì—…ì„ í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì¼ì§€ ìŠ¤ìŠ¤ë¡œ ê¹¨ìš°ì¹  ìˆ˜ ìˆì—ˆë‹¤ëŠ” ì ì—ì„œ êµ‰ì¥íˆ ê³ ë¬´ì ì´ë©°, ë§ì€ ê¹¨ë‹¬ìŒì„ ì–»ì„ ìˆ˜ ìˆì—ˆë˜ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤.

ë„ˆë¬´ ì¢‹ì€ íŒ€ì›ë¶„ë“¤ê³¼ í•¨ê»˜í•  ìˆ˜ ìˆì–´ì„œ ì •ë§ ì¢‹ì•˜ê³  ë‹¤ìŒ ëŒ€íšŒê°€ ë„ˆë¬´ë‚˜ë„ ê¸°ë‹¤ë ¤ì§€ë„¤ìš”.

ì €í¬ì˜ ì´ì•¼ê¸°ê°€ ì¡°ê¸ˆì´ë¼ë„ ë„ì›€ì´ ë˜ì—ˆê¸¸ ë°”ë¼ë©´ì„œ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤.

ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.


## ğŸ¤œ ë¶€ë¡: íŒ€ì›ë“¤ í•œë§ˆë””

### ì¢‹ì•˜ë˜ ì 

- ìš”í•œ: ì² ì €í•˜ê²Œ ê¸°ë¡ì„ í•˜ê³ , ì‹¤í—˜ê²°ê³¼ë¥¼ ê³µìœ í–ˆë˜ ì ì´ ê°€ì¥ ì˜ í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ ë‹¤ì–‘í•œ í˜‘ì—…íˆ´ì„ ë‘ê³  ì‚¬ìš©í•œ ê²ƒë³´ë‹¤ ê·¸ ê¸°ëŠ¥ë“¤ì„ ë…¸ì…˜ì—ë‹¤ê°€ ì „ë¶€ í†µí•©í•˜ì—¬ ì‚¬ìš©í•œê²Œ í˜¼ë€ì´ ì ì–´ ì˜ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
- í•˜ê²¸: ì‹¤í—˜ ìì²´ë¥¼ ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•˜ê³ , ì‹¤í—˜ ê³µìœ ê°€ ì˜ ë¬ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ì „ ì‹¤í—˜ì˜ ê²°ë¡ ì—ì„œ ë‹¤ìŒ ì‹¤í—˜ì€ ì–´ë–»ê²Œ í• ì§€ë¥¼ ì •í•œ ê²ƒë„ ì¢‹ì•˜ìŠµë‹ˆë‹¤.
- ì¤€ì˜: íŒ€ì› ê°„ì— ê³µìœ ê°€ ì˜ë˜ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ê¸°ë¡ì´ ì˜ë˜ë‹¤ë³´ë‹ˆ ì œê°€ í•˜ì§€ ì•Šì€ ì‹¤í—˜ì—ì„œë„ ì•„ì´ë””ì–´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
- ì§„ì›: íŒ€ì›ë“¤ë¼ë¦¬ ê²°ê³¼ ê³µìœ ì™€ ê¸°ë¡ì´ ì˜ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ì—´ì‹¬íˆ í•œ ë§Œí¼ ìµœì¢…ì ìœ¼ë¡œë„ ê´œì°®ì€ ê²°ê³¼ë¥¼ ì–»ì–´ì„œ ë§Œì¡±í•©ë‹ˆë‹¤.
- ë¯¼ìˆ˜: ì‹¤í—˜ì„ ë§ì´ í–ˆë˜ ê²ƒ. ì œì¶œ íšŸìˆ˜ë¥¼ ê½‰ ì±„ì›Œì„œ ì“´ê²Œ ì¢‹ì•˜ìŠµë‹ˆë‹¤.
- í¬ì˜: ì˜ê²¬ ê³µìœ ê°€ ì˜ ë˜ì–´ì„œ ë‚´ê°€ í•˜ì§€ ì•Šì€ ì‹¤í—˜ì—ì„œë„ ì§€ì‹ì„ ì–»ì„ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤. íŒ€ì›ë“¤ê³¼ ë©˜í† ë‹˜ì´ ì§€ì‹ì´ ë§ì•„ì„œ ì§„ì§œ ë¹ ë¥´ê²Œ ë°°ì› ìŠµë‹ˆë‹¤.
- ì§„ì„± : ê°ì ì‹¤í—˜ì— ìˆì–´ í†µì œë³€ì¸ ì„¤ì •ì„ ì² ì €íˆ í•˜ê³  ê¸°ë¡ì„ ì„¸ì„¸í•˜ê²Œ í•´, ì‹¤í—˜ì˜ íš¨ê³¼ë¥¼ ê³µìœ í•˜ê¸° ì¢‹ì•˜ìŠµë‹ˆë‹¤.

### ì•„ì‰¬ìš´ ì 

- ìš”í•œ: ì—¬ëŸ¬ ì‹¤í—˜ ì•„ì´ë””ì–´ë“¤ì´ ìˆì—ˆì§€ë§Œ, ë§‰íŒìœ¼ë¡œ ê°ˆ ìˆ˜ë¡ ì„±ëŠ¥ì— ì§‘ì°©í•˜ì—¬ í° ëª¨ë¸ë¡œë§Œ ì‹¤í—˜ì„ í•˜ëŠë¼ ëª¨ë“  ì•„ì´ë””ì–´ë“¤ì— ëŒ€í•´ì„œ ì‹¤í—˜ì„ í•˜ì§€ ëª»í–ˆë˜ ê²ƒì´ ì•„ì‰½ìŠµë‹ˆë‹¤.
- í•˜ê²¸ : ê°„ê·¹ì´ ì ì€ validation setì„ ê²°êµ­ì€ ëª»ì°¾ì•˜ë‹¤. â†’ ì–´ë–»ê²Œ ì°¾ì„ ìˆ˜ ìˆì„ì§€ëŠ” ì•„ì§ë„ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. ì„±ëŠ¥ë©´ì—ì„œë„ í° ë„ì›€ì´ ë˜ì§€ ì•Šì•„ì„œ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤.
- ì¤€ì˜: ëª¨ë¸ ì‘ì—…ì„ í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ê´€ë ¨í•´ì„œ ë§ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì•„ë³´ê³  ì‹¶ì—ˆì§€ë§Œ ê²°ê³¼ë¥¼ ì œëŒ€ë¡œ ë‚´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
- ì§„ì›: ëª¨ë¸ì„ ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì„ ë§ì´ í•˜ì§€ ëª»í•´ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ì§€ë§Œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì˜¬ë¦¬ëŠ”ë° í¬ê²Œ ê¸°ì—¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
- ë¯¼ìˆ˜: ëª¨ë¸ì„ íƒœìŠ¤í¬ì— ë§ê²Œ ë³´ë‹¤ ì ê·¹ì ìœ¼ë¡œ ë³€í˜•í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
- í¬ì˜: ëŒ€íšŒ ì´ˆê¸° ìƒí™œ ìŠ¤ì¼€ì¤„ì´ ê¼¬ì—¬ì„œ ì‹œê°„ ë‚­ë¹„ë¥¼ ë§ì´ í–ˆìŠµë‹ˆë‹¤. ìƒí™œ ìŠµê´€ì„ ì˜ ì¡ê³  ì‹œì‘í•˜ëŠ” ê²Œ ì¤‘ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
- ì§„ì„± : ìˆ˜ë™ì ìœ¼ë¡œ í•  ì¼ì„ ë°›ì•„ì„œ í•˜ê±°ë‚˜, ë‹¤ë¥¸ ì‚¬ëŒì˜ branchì— ë§ë¶™ì—¬ì„œ ì‘ì€ ì‹¤í—˜ë“¤ë§Œ í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ëŒ€íšŒì—ì„œëŠ” ë‚˜ë„ ì ê·¹ì ìœ¼ë¡œ ë…¼ë¬¸ ë“±ì—ì„œ ì•„ì´ë””ì–´ë¥¼ ì–»ì–´ì™€ í¼ì§í•˜ê²Œ(ë¹ ë¥´ê²Œ) ì •í™•í•˜ê²Œ êµ¬í˜„í•˜ëŠ” ì—°ìŠµì„ í•´ë³´ì.

### ê°œì„ í•  ì 

- ìš”í•œ: ì„±ëŠ¥ì€ ì–´ì°¨í”¼ ì˜¤ë¥¼ ê²ƒì´ê¸° ë•Œë¬¸ì—, ë¹„ìŠ·í•œ ì‹¤í—˜ì„ ì—¬ëŸ¬ë²ˆ í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ë– ì˜¬ë¦° ì•„ì´ë””ì–´ë“¤ì— ëŒ€í•´ ì „ë¶€ ì‹¤í—˜í•  ìˆ˜ ìˆë„ë¡ í•´ë´ì•¼ê² ìŠµë‹ˆë‹¤.
- í•˜ê²¸: ì¼ë‹¨ ê°•ì˜ë¶€í„° ë“¤ì–´ì„œ ë§¨ë•…ì— í—¤ë”©í•˜ì§€ ì•Šê¸°
- ì¤€ì˜: ê°•ì˜ ë¹ ë¥´ê²Œ ë“£ê¸°. ë°ì´í„° ë¹ ë¥´ê²Œ í›‘ì–´ë³´ê¸°. ëª¨ë¸ ëœ¯ì–´ë³´ê¸°.
- ì§„ì›: ëª¨ë¸ì„ ìˆ˜ì •í•´ë³´ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•˜ê³ , í™•ì‹¤í•˜ê²Œ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ê¸° ìœ„í•´ ì‹¤í—˜ì„ ì§„í–‰í•  ë•Œ ì¡°ê¸ˆ ë” ëª…í™•í•œ ê·¼ê±°ë‚˜ ë°©ë²•ì„ ë¯¸ë¦¬ ì¡°ì‚¬í•´ë³´ê³  ì§„í–‰í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ (e.g. ë…¼ë¬¸ ì½ì–´ì„œ train íšŸìˆ˜ ì •í•˜ê¸°).
- ë¯¼ìˆ˜: ì–´ë–¤ì‹ì˜ ë³€í˜•ì´ í•´ë‹¹ íƒœìŠ¤í¬ì— ì í•©í•œì§€ ë‹¨ìˆœ ì ìˆ˜ì™€ ëŠë‚Œ ì´ì™¸ì— ì •ëŸ‰ì ìœ¼ë¡œ ê²€ì¦í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ìƒê°í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
- í¬ì˜: ì²«ë‚  ì§„ì§œ ë¹ ë¥´ê²Œ ê°•ì˜ ë‹¤ ë“£ê¸°. 10ì‹œ~12ì‹œ ì‚¬ì´ ì‹œê°„ ì˜ ì´ìš©í•˜ê¸°. ì´ë™ í›„ ë°”ë¡œ ì‹œì‘í•˜ê¸°, 12ì‹œ ë„˜ìœ¼ë©´ ì§‘ì—ì„œ ê³µë¶€í•  ìƒê°í•˜ì§€ ë§ê¸°. EDAë¥¼ ë„ˆë¬´ ì˜¤ë˜ í•˜ê³  raw ë°ì´í„°ë¥¼ ë„ˆë¬´ ë§ì´ ë³´ë©´ ì‹œê°„ ë‚­ë¹„ì¼ìˆ˜ë„.
- ì§„ì„± : í›„ë°˜ë¶€ì— ê¸°ë¡ì„ í•˜ëŠ” ë°ì— ì‹ ê²½ì„ ë§ì´ ëª»ì¼ìŠµë‹ˆë‹¤. ê²°êµ­ ë‚¨ëŠ”ê±´ ê¸°ë¡ì´ë‹ˆ ë‹¤ìŒ ëŒ€íšŒ ê¸´ ê¸°ê°„ë™ì•ˆì—ë„ ê¾¸ì¤€íˆ ê¸°ë¡í•˜ì.